# -*- coding: utf-8 -*-
"""Recommendation_model2.ipynb..ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GLWkIM20pRuhtDOTGY4rKM_2RDnZ5iBl
"""

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator
 
from pyspark.sql import functions as F
 
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
from pyspark.storagelevel import StorageLevel
from pyspark.sql.functions import col

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

PYSPARK_CLI = True # conditional statement to run only at shell
if PYSPARK_CLI:
 sc = SparkContext.getOrCreate()
 spark = SparkSession(sc)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
# File location and type
file_location = "/tmp/used_cars_data.csv"
#file_location = "/Users/kangjoin/Downloads/used_cars_data 2.csv"
file_type = "csv"
 
# CSV options
infer_schema = "TRUE"
first_row_is_header = "TRUE"
delimiter = ","
 
# The applied options are for CSV files. For other file types, these will be ignored.
df = spark.read.format(file_type) \
  .option("inferSchema", infer_schema) \
  .option("header", first_row_is_header) \
  .option("sep", delimiter) \
  .load(file_location)
 
df.show()

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
temp_table_name = "used_cars_data_csv"
 
df.createOrReplaceTempView(temp_table_name)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.
# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.
# To do so, choose your table name and uncomment the bottom line.
 
permanent_table_name = "used_cars_data_csv"
 
# df.write.format("parquet").saveAsTable(permanent_table_name)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_data = spark.sql("Select * from used_cars_data_csv")
df_data.show()

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_data.dtypes

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
from pyspark.sql.functions import col
from functools import reduce
import pyspark.sql.functions as F
from pyspark.sql.types import DoubleType,IntegerType

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new = df_data.select(col('engine_displacement'),col('frame_damaged') ,col('has_accidents') ,col('horsepower'),col('isCab'),col('is_new'),col('mileage'),col('power'),col('price'),col('seller_rating'),col('sp_id'),col('make_name'),col('daysonmarket'))

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new = df_new.withColumn("engine_displacement",col("engine_displacement").cast(DoubleType()))
df_new = df_new.withColumn("horsepower",col("horsepower").cast(DoubleType()))
df_new = df_new.withColumn("power",col("power").cast(DoubleType()))
df_new = df_new.withColumn("mileage",col("mileage").cast(IntegerType()))
df_new = df_new.withColumn("price",col("price").cast(IntegerType()))
df_new = df_new.withColumn("seller_rating",col("seller_rating").cast(DoubleType())) 
df_new = df_new.withColumn("daysonmarket",col("daysonmarket").cast(IntegerType()))

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new.printSchema()
df_new.show()

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
cols = ['is_new']
col2 = ['frame_damaged','has_accidents','isCab']

# Commented out IPython magic to ensure Python compatibility.
# %pyspark

df_new= reduce(lambda df_new, c: df_new.withColumn(c, F.when(df_new[c] == 'False', 0).otherwise(1)), cols, df_new)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new=  df_new.na.fill(value=0,subset=["mileage"])

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new = reduce(lambda df_new, c: df_new.withColumn(c, F.when(df_new[c]== 'False', 2).when(df_new[c]== 'True', 0).otherwise(1)), col2, df_new)
df_new= df_new.na.fill(value=0,subset=["engine_displacement"])
df_new= df_new.na.fill(value=0,subset=["horsepower"])
df_new= df_new.na.fill(value=0,subset=["power"])

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new= df_new.na.fill(value=0,subset=["seller_rating"])
df_new= df_new.na.fill(value=0,subset=["price"])

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new.show()

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new = df_new.withColumn("is_new",col("is_new").cast(IntegerType()))
df_new = df_new.withColumn("frame_damaged",col("frame_damaged").cast(IntegerType()))
df_new = df_new.withColumn("has_accidents",col("has_accidents").cast(IntegerType()))
df_new = df_new.withColumn("isCab",col("isCab").cast(IntegerType()))
df_new = df_new.withColumn("sp_id",col("sp_id").cast(IntegerType()))

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new = df_new.select('*').where(col("price")>0)
df_new = df_new.select('*').where(col("price")<10000000)
 
df_new = df_new.select('*').where(col("engine_displacement")>0)
df_new = df_new.select('*').where(col("horsepower")>0)
df_new = df_new.select('*').where(col("sp_id")>=0)
df_new = df_new.select('*').where(col("daysonmarket")>=0)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
df_new.printSchema()
df_new.show()

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
data = df_new.select("sp_id","make_name","daysonmarket","seller_rating").distinct()
data.show()

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
splits = data.randomSplit([0.7, 0.3])
train = splits[0].withColumnRenamed("seller_rating","label")
test = splits[1].withColumnRenamed("seller_rating","trueLabel")
train_rows = train.count()
test_rows = test.count()
print("Training Rows:", train_rows, "Testing Rows:", test_rows)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
als = ALS(userCol = "sp_id",itemCol = "daysonmarket", ratingCol = "label")
#als = ALS(maxIter=5, regParam=0.01, userCol="sp_id", itemCol="daysonmarket", ratingCol="label")
#model = als.fit(train)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
paramGrid = ParamGridBuilder() \
                    .addGrid(als.rank, [1]) \
                    .addGrid(als.maxIter, [5]) \
                    .addGrid(als.regParam, [0.3]) \
                    .addGrid(als.alpha, [2.0]) \
                    .build()

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
cv = TrainValidationSplit(estimator=als, evaluator=RegressionEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)
model = cv.fit(train)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
prediction = model.transform(test)
 
# Remove NaN values from prediction (due to SPARK-14489) [1]
prediction = prediction.filter(prediction.prediction != float('nan'))
 
# Round floats to whole numbers
prediction = prediction.withColumn("prediction", F.abs(F.round(prediction["prediction"],0)))
 
prediction.select("sp_id", "make_name", "prediction", "trueLabel").show(30, truncate=False)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
evaluator = RegressionEvaluator(labelCol="trueLabel", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(prediction)
print ("Root Mean Square Error (RMSE):", rmse)

# Commented out IPython magic to ensure Python compatibility.
# %pyspark
evaluator = RegressionEvaluator(labelCol="trueLabel", predictionCol="prediction", metricName="r2")
r2 = evaluator.evaluate(prediction)
print ("Coefficient of Determination (R2):", r2)
